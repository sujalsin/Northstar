{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2ZehZ6mEzJ76cmeWntq1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujalsin/Northstar/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing"
      ],
      "metadata": {
        "id": "inhV2yJ-N_BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data from - https://www.kaggle.com/datasets/bryanpark/nana-dataset?resource=download"
      ],
      "metadata": {
        "id": "BcHtO3M5PXND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From NaNa dataset- Upload **train.src, train.tgt, test.src, test.tgt**\n",
        "\n",
        "Also, upload **Homograph.txt, checkpoint20.npz**"
      ],
      "metadata": {
        "id": "yQpPfS8JPenA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWnA3I1UNtRw"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "names = open(\"/content/train.src\", \"r\")\n",
        "nats = open(\"/content/train.tgt\", \"r\")\n",
        "with open(\"/content/train.csv\", \"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow([\"Names\", \"Nationality\"])\n",
        "  for name, nat in zip(names, nats):\n",
        "    writer.writerow([name.replace(\"\\n\", \"\"), nat.replace(\"\\n\", \"\")])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = open(\"/content/test.src\", \"r\")\n",
        "nats = open(\"/content/test.tgt\", \"r\")\n",
        "dic = defaultdict()\n",
        "dic2 = defaultdict()\n",
        "with open(\"/content/test.csv\", \"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow([\"Names\", \"Nationality\"])\n",
        "  for name, nat in zip(names, nats):\n",
        "    #print(name.replace(\"\\n\", \"\").split(' ')[-1], nat)\n",
        "    writer.writerow([name.replace(\"\\n\", \"\"), nat.replace(\"\\n\", \"\")])\n",
        "    dic[name.replace(\"\\n\", \"\").split(' ')[-1]] = nat.replace(\"\\n\", \"\")\n",
        "    dic2[name.replace(\"\\n\", \"\")] = nat.replace(\"\\n\", \"\")"
      ],
      "metadata": {
        "id": "4JWLca3vOJuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lst_of_country():\n",
        "  dic = defaultdict()\n",
        "  country_vector = defaultdict()\n",
        "  i = 0\n",
        "  with open(\"/content/train.csv\", 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    for line in reader:\n",
        "      country = line[1].replace(\"\\n\", \"\")\n",
        "      if country in dic:\n",
        "        dic[country].append(line[0].split(\" \")[-1])\n",
        "      else:\n",
        "        country_vector[country] = i\n",
        "        dic[country] = [line[0].split(\" \")[-1]]\n",
        "        i += 1\n",
        "  return country_vector"
      ],
      "metadata": {
        "id": "oZnLk1Z9N9wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "country_vector = lst_of_country()\n",
        "for key in country_vector.keys():\n",
        "  print(key, country_vector[key])"
      ],
      "metadata": {
        "id": "q9VysC3DN-gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NUMPY"
      ],
      "metadata": {
        "id": "gf2oFmv-OX1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import inflect\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "_inflect = inflect.engine()\n",
        "_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n",
        "_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n",
        "_pounds_re = re.compile(r'Â£([0-9\\,]*[0-9]+)')\n",
        "_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n",
        "_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n",
        "_number_re = re.compile(r'[0-9]+')\n",
        "\n",
        "\n",
        "def _remove_commas(m):\n",
        "    return m.group(1).replace(',', '')\n",
        "\n",
        "\n",
        "def _expand_decimal_point(m):\n",
        "    return m.group(1).replace('.', ' point ')\n",
        "\n",
        "\n",
        "def _expand_dollars(m):\n",
        "    match = m.group(1)\n",
        "    parts = match.split('.')\n",
        "    if len(parts) > 2:\n",
        "        return match + ' dollars'    # Unexpected format\n",
        "    dollars = int(parts[0]) if parts[0] else 0\n",
        "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
        "    if dollars and cents:\n",
        "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
        "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
        "        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n",
        "    elif dollars:\n",
        "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
        "        return '%s %s' % (dollars, dollar_unit)\n",
        "    elif cents:\n",
        "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
        "        return '%s %s' % (cents, cent_unit)\n",
        "    else:\n",
        "        return 'zero dollars'\n",
        "\n",
        "\n",
        "def _expand_ordinal(m):\n",
        "    return _inflect.number_to_words(m.group(0))\n",
        "\n",
        "\n",
        "def _expand_number(m):\n",
        "    num = int(m.group(0))\n",
        "    if num > 1000 and num < 3000:\n",
        "        if num == 2000:\n",
        "            return 'two thousand'\n",
        "        elif num > 2000 and num < 2010:\n",
        "            return 'two thousand ' + _inflect.number_to_words(num % 100)\n",
        "        elif num % 100 == 0:\n",
        "            return _inflect.number_to_words(num // 100) + ' hundred'\n",
        "        else:\n",
        "            return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n",
        "    else:\n",
        "        return _inflect.number_to_words(num, andword='')\n",
        "\n",
        "\n",
        "def normalize_numbers(text):\n",
        "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
        "    text = re.sub(_pounds_re, r'\\1 pounds', text)\n",
        "    text = re.sub(_dollars_re, _expand_dollars, text)\n",
        "    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
        "    text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
        "    text = re.sub(_number_re, _expand_number, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "4APURXBoOPWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**G2P**"
      ],
      "metadata": {
        "id": "I8sjhYvVOhso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/homograph.txt\")\n",
        "homograph = dict()\n",
        "for line in f.readlines()[10:]:\n",
        "  if line.startswith(\"#\"):continue\n",
        "  l = line.split(\"|\")\n",
        "  print(l)\n",
        "  homograph[l[0].lower()] = (l[1].split(), l[2].split(), l[3].replace(\"\\\\\\n\", \"\"))\n",
        "print(len(homograph))"
      ],
      "metadata": {
        "id": "2MBpQX9vOai4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "variables = np.load(\"/checkpoint20.npz\")\n",
        "print(variables['enc_emb'])"
      ],
      "metadata": {
        "id": "ITl6ZW9IOeQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import cmudict\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "word_tokenize = TweetTokenizer().tokenize\n",
        "import numpy as np\n",
        "import codecs\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "from builtins import str as unicode\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger.zip')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "try:\n",
        "    nltk.data.find('corpora/cmudict.zip')\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "\n",
        "\n",
        "\n",
        "def construct_homograph_dictionary():\n",
        "    homograph2features = dict()\n",
        "    f = open(\"/homograph.txt\")\n",
        "    for line in f.readlines()[10:]:\n",
        "        #print(line)\n",
        "        if line.startswith(\"#\"): continue # comment\n",
        "        headword, pron1, pron2, pos1 = line.split(\"|\")[0], line.split(\"|\")[1],line.split(\"|\")[2],line.split(\"|\")[3].replace(\"\\\\\\n\", \"\")\n",
        "        homograph2features[headword.lower()] = (pron1.split(), pron2.split(), pos1)\n",
        "    return homograph2features\n",
        "\n",
        "# def segment(text):\n",
        "#     '''\n",
        "#     Splits text into `tokens`.\n",
        "#     :param text: A string.\n",
        "#     :return: A list of tokens (string).\n",
        "#     '''\n",
        "#     print(text)\n",
        "#     text = re.sub('([.,?!]( |$))', r' \\1', text)\n",
        "#     print(text)\n",
        "#     return text.split()\n",
        "\n",
        "class G2p(object):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "        self.phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + ['AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0',\n",
        "                                                             'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH',\n",
        "                                                             'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1',\n",
        "                                                             'EY2', 'F', 'G', 'HH',\n",
        "                                                             'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L',\n",
        "                                                             'M', 'N', 'NG', 'OW0', 'OW1',\n",
        "                                                             'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH',\n",
        "                                                             'UH0', 'UH1', 'UH2', 'UW',\n",
        "                                                             'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH']\n",
        "        self.g2idx = {g: idx for idx, g in enumerate(self.graphemes)}\n",
        "        self.idx2g = {idx: g for idx, g in enumerate(self.graphemes)}\n",
        "\n",
        "        self.p2idx = {p: idx for idx, p in enumerate(self.phonemes)}\n",
        "        self.idx2p = {idx: p for idx, p in enumerate(self.phonemes)}\n",
        "\n",
        "        self.cmu = cmudict.dict()\n",
        "        self.load_variables()\n",
        "        self.homograph2features = construct_homograph_dictionary()\n",
        "\n",
        "    def load_variables(self):\n",
        "        self.variables = np.load(\"/checkpoint20.npz\")\n",
        "        self.enc_emb = self.variables[\"enc_emb\"]  # (29, 64). (len(graphemes), emb)\n",
        "        self.enc_w_ih = self.variables[\"enc_w_ih\"]  # (3*128, 64)\n",
        "        self.enc_w_hh = self.variables[\"enc_w_hh\"]  # (3*128, 128)\n",
        "        self.enc_b_ih = self.variables[\"enc_b_ih\"]  # (3*128,)\n",
        "        self.enc_b_hh = self.variables[\"enc_b_hh\"]  # (3*128,)\n",
        "\n",
        "        self.dec_emb = self.variables[\"dec_emb\"]  # (74, 64). (len(phonemes), emb)\n",
        "        self.dec_w_ih = self.variables[\"dec_w_ih\"]  # (3*128, 64)\n",
        "        self.dec_w_hh = self.variables[\"dec_w_hh\"]  # (3*128, 128)\n",
        "        self.dec_b_ih = self.variables[\"dec_b_ih\"]  # (3*128,)\n",
        "        self.dec_b_hh = self.variables[\"dec_b_hh\"]  # (3*128,)\n",
        "        self.fc_w = self.variables[\"fc_w\"]  # (74, 128)\n",
        "        self.fc_b = self.variables[\"fc_b\"]  # (74,)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x)) # mathematical logistic function // for the activation of the neural network\n",
        "\n",
        "    def grucell(self, x, h, w_ih, w_hh, b_ih, b_hh):\n",
        "        rzn_ih = np.matmul(x, w_ih.T) + b_ih # matrix_product of two array\n",
        "        rzn_hh = np.matmul(h, w_hh.T) + b_hh\n",
        "\n",
        "        rz_ih, n_ih = rzn_ih[:, :rzn_ih.shape[-1] * 2 // 3], rzn_ih[:, rzn_ih.shape[-1] * 2 // 3:]\n",
        "        rz_hh, n_hh = rzn_hh[:, :rzn_hh.shape[-1] * 2 // 3], rzn_hh[:, rzn_hh.shape[-1] * 2 // 3:]\n",
        "\n",
        "        rz = self.sigmoid(rz_ih + rz_hh)\n",
        "        r, z = np.split(rz, 2, -1)\n",
        "\n",
        "        n = np.tanh(n_ih + r * n_hh)\n",
        "        h = (1 - z) * n + z * h\n",
        "\n",
        "        return h\n",
        "\n",
        "    def gru(self, x, steps, w_ih, w_hh, b_ih, b_hh, h0=None):\n",
        "        if h0 is None:\n",
        "            h0 = np.zeros((x.shape[0], w_hh.shape[1]), np.float32)\n",
        "        h = h0  # initial hidden state\n",
        "        outputs = np.zeros((x.shape[0], steps, w_hh.shape[1]), np.float32)\n",
        "        for t in range(steps):\n",
        "            h = self.grucell(x[:, t, :], h, w_ih, w_hh, b_ih, b_hh)  # (b, h)\n",
        "            outputs[:, t, ::] = h\n",
        "        return outputs\n",
        "\n",
        "    def encode(self, word):\n",
        "        chars = list(word) + [\"</s>\"]\n",
        "        x = [self.g2idx.get(char, self.g2idx[\"<unk>\"]) for char in chars]\n",
        "        x = np.take(self.enc_emb, np.expand_dims(x, 0), axis=0)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, word):\n",
        "        # encoder\n",
        "        enc = self.encode(word)\n",
        "        enc = self.gru(enc, len(word) + 1, self.enc_w_ih, self.enc_w_hh,\n",
        "                       self.enc_b_ih, self.enc_b_hh, h0=np.zeros((1, self.enc_w_hh.shape[-1]), np.float32))\n",
        "        last_hidden = enc[:, -1, :]\n",
        "\n",
        "        # decoder\n",
        "        dec = np.take(self.dec_emb, [2], axis=0)  # 2: <s>\n",
        "        h = last_hidden\n",
        "\n",
        "        preds = []\n",
        "        for i in range(20):\n",
        "            h = self.grucell(dec, h, self.dec_w_ih, self.dec_w_hh, self.dec_b_ih, self.dec_b_hh)  # (b, h)\n",
        "            logits = np.matmul(h, self.fc_w.T) + self.fc_b\n",
        "            pred = logits.argmax()\n",
        "            if pred == 3: break  # 3: </s>\n",
        "            preds.append(pred)\n",
        "            dec = np.take(self.dec_emb, [pred], axis=0)\n",
        "\n",
        "        preds = [self.idx2p.get(idx, \"<unk>\") for idx in preds]\n",
        "        return preds\n",
        "\n",
        "    def __call__(self, text):\n",
        "        # preprocessing\n",
        "        text = unicode(text)\n",
        "        text = normalize_numbers(text)\n",
        "        text = ''.join(char for char in unicodedata.normalize('NFD', text) #Convert the text into its decomposed form\n",
        "                       if unicodedata.category(char) != 'Mn')  # Strip accents\n",
        "        text = text.lower()\n",
        "        text = re.sub(\"[^ a-z'.,?!\\-]\", \"\", text)\n",
        "        text = text.replace(\"i.e.\", \"that is\")\n",
        "        text = text.replace(\"e.g.\", \"for example\")\n",
        "\n",
        "        # tokenization\n",
        "        words = word_tokenize(text)\n",
        "        tokens = pos_tag(words)  # tuples of (word, tag)\n",
        "\n",
        "        # steps\n",
        "        prons = []\n",
        "        for word, pos in tokens:\n",
        "            if re.search(\"[a-z]\", word) is None:\n",
        "                pron = [word]\n",
        "\n",
        "            elif word in self.homograph2features:  # Check homograph\n",
        "                pron1, pron2, pos1 = self.homograph2features[word]\n",
        "                if pos.startswith(pos1):\n",
        "                    pron = pron1\n",
        "                else:\n",
        "                    pron = pron2\n",
        "            elif word in self.cmu:  # lookup CMU dict\n",
        "                pron = self.cmu[word][0]\n",
        "            else: # predict for oov\n",
        "                pron = self.predict(word)\n",
        "\n",
        "            prons.extend(pron)\n",
        "            prons.extend([\" \"])\n",
        "\n",
        "        return prons[:-1]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    texts = [\"I have $250 in my pocket.\", # number -> spell-out\n",
        "             \"popular pets, e.g. cats and dogs\", # e.g. -> for example\n",
        "             \"I refuse to collect the refuse around here.\", # homograph\n",
        "             \"I'm an activationist.\"] # newly coined word\n",
        "    g2p = G2p()\n",
        "    for text in texts:\n",
        "        out = g2p(text)\n",
        "        print(out)\n",
        "    text = \"Robert Pruzan\"\n",
        "    print(g2p(text))"
      ],
      "metadata": {
        "id": "-pXRTjZtOo7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name2Nationality**"
      ],
      "metadata": {
        "id": "t-WkH3mTO3LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.traitlets import default\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "name_to_country2 = defaultdict()\n",
        "name_to_index2 = defaultdict()\n",
        "g2p = G2p()\n",
        "def get_names():\n",
        "  lst = []\n",
        "  with open(\"/content/train.csv\", 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    i = 0\n",
        "    for line in reader:\n",
        "      name_to_country2[\"\".join(g2p(line[0]))] = line[1]\n",
        "      name_to_index2[\"\".join(g2p(line[0]))] = i\n",
        "      i += 1\n",
        "      lst.append(\"\".join(g2p(line[0])))\n",
        "      if i == 1000:\n",
        "        break\n",
        "  return lst\n",
        "\n",
        "names = get_names()"
      ],
      "metadata": {
        "id": "-QsJ1fsQOwu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epochs, vector_size, window):\n",
        "    documents = [TaggedDocument(list(doc), [i]) for i, doc in enumerate(names)]\n",
        "    model = Doc2Vec(documents, epochs=epochs, vector_size=vector_size, window=window, workers=1)\n",
        "    return model\n",
        "model2 = train_model(50, 50, 4)\n",
        "\n",
        "name_to_vector2 = defaultdict()\n",
        "\n",
        "for i in name_to_index2.keys():\n",
        "  name_to_vector2[i] = model2.docvecs[name_to_index2[i]]"
      ],
      "metadata": {
        "id": "vo97UyoWO9d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "lst2 = []\n",
        "y2 = []\n",
        "country_vector = lst_of_country()\n",
        "for i in name_to_index2.keys():\n",
        "  name_to_vector2[i] = model2.docvecs[name_to_index2[i]]\n",
        "  y2.append(country_vector[name_to_country2[i]])\n",
        "  lst2.append(name_to_vector2[i].flatten())\n",
        "neigh2 = KNeighborsClassifier(n_neighbors=30)\n",
        "neigh2.fit(lst2, y2)\n",
        "n = \"\".join(g2p('Li you'))\n",
        "name_vector2 = model2.infer_vector(n).flatten()\n",
        "print(neigh2.predict([name_vector2]))\n",
        "for i in country_vector.keys():\n",
        "  print(i, country_vector[i])"
      ],
      "metadata": {
        "id": "vLmYW5CYPKyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Check Accuracy"
      ],
      "metadata": {
        "id": "a1-wSaJSPQIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "j = 0\n",
        "for key in list(dic2.keys())[:2000]:\n",
        "\n",
        "  phoneme = g2p(key)\n",
        "  name_vector = model2.infer_vector(\"\".join(phoneme)).flatten()\n",
        "  vector = neigh2.predict([name_vector])\n",
        "  if vector[0] == country_vector[dic2[key]]:\n",
        "    i += 1\n",
        "  j += 1\n",
        "\n",
        "print(float(i/j))"
      ],
      "metadata": {
        "id": "I-2WPBsKPLsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}